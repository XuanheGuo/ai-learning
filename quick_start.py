#!/usr/bin/env python3
"""
æ•°å­¦è§„å¾‹å‘ç°å¤§æ¨¡å‹ - å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
é€‚ç”¨äºGoogle Colab + TPUç¯å¢ƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# æ£€æŸ¥TPU
try:
    import torch_xla.core.xla_model as xm
    TPU_AVAILABLE = True
    print("âœ… TPUå¯ç”¨")
except ImportError:
    TPU_AVAILABLE = False
    print("âš ï¸  TPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU/GPU")

class MathPatternModel(nn.Module):
    """æ•°å­¦è§„å¾‹å‘ç°æ¨¡å‹"""
    
    def __init__(self, hidden_size=1024, num_layers=12, matrix_dim=512, vector_dim=256):
        super().__init__()
        self.hidden_size = hidden_size
        self.matrix_dim = matrix_dim
        self.vector_dim = vector_dim
        
        # åµŒå…¥å±‚
        self.matrix_embedding = nn.Linear(matrix_dim * matrix_dim, hidden_size)
        self.vector_embedding = nn.Linear(vector_dim, hidden_size)
        self.symbol_embedding = nn.Embedding(1000, hidden_size)
        self.relation_embedding = nn.Embedding(20, hidden_size)
        
        # Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=16,
            dim_feedforward=4096,
            dropout=0.1,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # æ¨¡å¼å‘ç°
        self.pattern_encoder = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Linear(hidden_size // 2, 8)
        )
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(hidden_size, hidden_size)
        self.relation_predictor = nn.Linear(hidden_size, 20)
        
    def forward(self, matrices, vectors, symbols, relations):
        batch_size, seq_len = matrices.shape[:2]
        
        # çŸ©é˜µåµŒå…¥
        matrices_flat = matrices.view(batch_size, seq_len, -1)
        matrix_emb = self.matrix_embedding(matrices_flat)
        
        # å‘é‡åµŒå…¥
        vector_emb = self.vector_embedding(vectors)
        
        # ç¬¦å·å’Œå…³ç³»åµŒå…¥
        symbol_emb = self.symbol_embedding(symbols)
        relation_emb = self.relation_embedding(relations)
        
        # ç»„åˆåµŒå…¥
        combined_emb = matrix_emb + vector_emb + symbol_emb + relation_emb
        
        # Transformerå¤„ç†
        hidden_states = self.transformer(combined_emb.transpose(0, 1)).transpose(0, 1)
        
        # æ¨¡å¼å‘ç°
        pattern_logits = self.pattern_encoder(hidden_states)
        
        # è¾“å‡º
        output = self.output_projection(hidden_states)
        relation_output = self.relation_predictor(hidden_states)
        
        return {
            'hidden_states': output,
            'pattern_logits': pattern_logits,
            'relation_output': relation_output
        }

def generate_lattice_data(num_samples=1000, matrix_dim=512, vector_dim=256):
    """ç”Ÿæˆæ ¼å¯†ç å­¦æ•°æ®"""
    print(f"ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")
    
    # ç”ŸæˆéšæœºçŸ©é˜µï¼ˆæ¨¡æ‹Ÿæ ¼åŸºï¼‰
    matrices = torch.randn(num_samples, matrix_dim, matrix_dim)
    
    # ç”Ÿæˆéšæœºå‘é‡ï¼ˆæ¨¡æ‹Ÿæ¶ˆæ¯ï¼‰
    vectors = torch.randn(num_samples, vector_dim)
    
    # ç”Ÿæˆæ•°å­¦ç¬¦å·
    symbols = torch.randint(0, 1000, (num_samples,))
    
    # ç”Ÿæˆå…³ç³»ç±»å‹
    relations = torch.randint(0, 20, (num_samples,))
    
    return matrices, vectors, symbols, relations

def train_model(model, num_epochs=50, batch_size=32):
    """è®­ç»ƒæ¨¡å‹"""
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # é€‰æ‹©è®¾å¤‡
    if TPU_AVAILABLE:
        device = xm.xla_device()
        print("ä½¿ç”¨TPUè®­ç»ƒ")
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    
    pattern_loss_fn = nn.CrossEntropyLoss()
    relation_loss_fn = nn.CrossEntropyLoss()
    
    losses = []
    
    for epoch in range(num_epochs):
        model.train()
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        matrices, vectors, symbols, relations = generate_lattice_data(batch_size)
        relation_labels = torch.randint(0, 20, (batch_size,))
        pattern_labels = torch.randint(0, 8, (batch_size,))
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        matrices = matrices.to(device)
        vectors = vectors.to(device)
        symbols = symbols.to(device)
        relations = relations.to(device)
        relation_labels = relation_labels.to(device)
        pattern_labels = pattern_labels.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(matrices, vectors, symbols, relations)
        
        # è®¡ç®—æŸå¤±
        pattern_loss = pattern_loss_fn(outputs['pattern_logits'], pattern_labels)
        relation_loss = relation_loss_fn(outputs['relation_output'], relation_labels)
        total_loss = pattern_loss + relation_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        
        if TPU_AVAILABLE:
            xm.optimizer_step(optimizer)
        else:
            optimizer.step()
        
        losses.append(total_loss.item())
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return model

def analyze_patterns(model, matrices, vectors):
    """åˆ†ææ•°å­¦æ¨¡å¼"""
    print("åˆ†ææ•°å­¦æ¨¡å¼...")
    
    if TPU_AVAILABLE:
        device = xm.xla_device()
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model.eval()
    with torch.no_grad():
        batch_size = matrices.shape[0]
        symbols = torch.randint(0, 1000, (batch_size,)).to(device)
        relations = torch.randint(0, 20, (batch_size,)).to(device)
        
        matrices = matrices.to(device)
        vectors = vectors.to(device)
        
        outputs = model(matrices, vectors, symbols, relations)
        
        pattern_probs = F.softmax(outputs['pattern_logits'], dim=-1)
        relation_probs = F.softmax(outputs['relation_output'], dim=-1)
        
        return {
            'pattern_probabilities': pattern_probs,
            'relation_probabilities': relation_probs,
            'hidden_states': outputs['hidden_states']
        }

def visualize_results(results):
    """å¯è§†åŒ–åˆ†æç»“æœ"""
    print("ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æ¨¡å¼æ¦‚ç‡åˆ†å¸ƒ
    pattern_probs = results['pattern_probabilities'].cpu().numpy()
    im1 = axes[0,0].imshow(pattern_probs, cmap='viridis', aspect='auto')
    axes[0,0].set_title('æ¨¡å¼æ¦‚ç‡åˆ†å¸ƒ', fontsize=14)
    axes[0,0].set_xlabel('æ¨¡å¼ç±»å‹')
    axes[0,0].set_ylabel('æ ·æœ¬')
    plt.colorbar(im1, ax=axes[0,0])
    
    # å…³ç³»æ¦‚ç‡åˆ†å¸ƒ
    relation_probs = results['relation_probabilities'].cpu().numpy()
    im2 = axes[0,1].imshow(relation_probs, cmap='plasma', aspect='auto')
    axes[0,1].set_title('å…³ç³»æ¦‚ç‡åˆ†å¸ƒ', fontsize=14)
    axes[0,1].set_xlabel('å…³ç³»ç±»å‹')
    axes[0,1].set_ylabel('æ ·æœ¬')
    plt.colorbar(im2, ax=axes[0,1])
    
    # éšè—çŠ¶æ€å¯è§†åŒ–
    hidden_states = results['hidden_states'].cpu().numpy()
    im3 = axes[1,0].imshow(hidden_states[0], cmap='coolwarm', aspect='auto')
    axes[1,0].set_title('éšè—çŠ¶æ€ (ç¬¬ä¸€ä¸ªæ ·æœ¬)', fontsize=14)
    axes[1,0].set_xlabel('éšè—ç»´åº¦')
    axes[1,0].set_ylabel('åºåˆ—é•¿åº¦')
    plt.colorbar(im3, ax=axes[1,0])
    
    # æ¨¡å¼åˆ†å¸ƒç»Ÿè®¡
    pattern_means = pattern_probs.mean(axis=0)
    axes[1,1].bar(range(len(pattern_means)), pattern_means, color='skyblue')
    axes[1,1].set_title('å¹³å‡æ¨¡å¼åˆ†å¸ƒ', fontsize=14)
    axes[1,1].set_xlabel('æ¨¡å¼ç±»å‹')
    axes[1,1].set_ylabel('å¹³å‡æ¦‚ç‡')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("å¯è§†åŒ–å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§  æ•°å­¦è§„å¾‹å‘ç°å¤§æ¨¡å‹ - æ ¼å¯†ç å­¦åº”ç”¨")
    print("=" * 60)
    
    # 1. åˆ›å»ºæ¨¡å‹
    print("\n1ï¸âƒ£ åˆ›å»ºæ¨¡å‹...")
    model = MathPatternModel(
        hidden_size=1024,
        num_layers=12,
        matrix_dim=512,
        vector_dim=256
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 2. è®­ç»ƒæ¨¡å‹
    print("\n2ï¸âƒ£ è®­ç»ƒæ¨¡å‹...")
    trained_model = train_model(model, num_epochs=50, batch_size=32)
    
    # 3. æµ‹è¯•æ¨¡å‹
    print("\n3ï¸âƒ£ æµ‹è¯•æ¨¡å‹...")
    test_matrices = torch.randn(4, 512, 512)
    test_vectors = torch.randn(4, 256)
    
    results = analyze_patterns(trained_model, test_matrices, test_vectors)
    
    # 4. å¯è§†åŒ–ç»“æœ
    print("\n4ï¸âƒ£ å¯è§†åŒ–ç»“æœ...")
    visualize_results(results)
    
    # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n5ï¸âƒ£ åˆ†æç»Ÿè®¡:")
    print(f"æ¨¡å¼æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶: {results['pattern_probabilities'].shape}")
    print(f"å…³ç³»æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶: {results['relation_probabilities'].shape}")
    print(f"éšè—çŠ¶æ€å½¢çŠ¶: {results['hidden_states'].shape}")
    
    print("\nâœ… åˆ†æå®Œæˆ!")
    print("=" * 60)
    
    return trained_model, results

if __name__ == "__main__":
    model, results = main()
